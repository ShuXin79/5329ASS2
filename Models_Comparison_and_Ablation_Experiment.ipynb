{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Models Comparison and Ablation Experiment"
      ],
      "metadata": {
        "id": "ZacMxlfJrFpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The code in this part is only used for my model performance comparison and ablation experiments. **The purpose of uploading this code is only to prove that we have done these things. For the model implementation and evaluation code of this assignment, please refer to CNN_Transformer_Xin.ipynb. Please DO NOT look at this part of the code first!** Please refer to the code in this section to see how we performed the ablation experiments and which models and hyperparameters we compared."
      ],
      "metadata": {
        "id": "_ZO1JGjG_niT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation and Package Import"
      ],
      "metadata": {
        "id": "wA3IAdLzrWpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Clone data and images from GitHub"
      ],
      "metadata": {
        "id": "TA3iYQB_rkmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFHedtOKKFy",
        "outputId": "21de8083-099f-46d2-f31b-9d35b0bc64d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5329ASS2'...\n",
            "remote: Enumerating objects: 40017, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 40017 (delta 4), reused 10 (delta 2), pack-reused 40005\u001b[K\n",
            "Receiving objects: 100% (40017/40017), 393.03 MiB | 18.23 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Updating files: 100% (40004/40004), done.\n"
          ]
        }
      ],
      "source": [
        "# All the data were saved in my GitHub, it's easy to get them all by running this code.\n",
        "!git clone https://github.com/ShuXin79/5329ASS2.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Libraries install and import"
      ],
      "metadata": {
        "id": "YvxiD0sdrBZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKFrt_r5LuZY",
        "outputId": "69152973-4178-4637-c082-fb909ae6c153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "# Make sure your environment contains these libraries.\n",
        "!pip install torch pandas pillow\n",
        "!pip install torchvision\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BxXuABzVi6yf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "# We used the pre-trained model BERT for our NLP task\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "# sklearn is only used for splitting the train-validation sets as well as calculating the f1-score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Reading Class"
      ],
      "metadata": {
        "id": "Vwtf_PzDsAF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vBzfzq5i1Hdi"
      },
      "outputs": [],
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "  # This class is to convert the input data into the format we want to see.\n",
        "  \n",
        "  def __init__(self, df, img_dir, transform=None, max_length=128):\n",
        "    # df: the dataframe of our input data\n",
        "    # img_dir: saved the path to the images\n",
        "    # transform: determine the format of input images data\n",
        "    # max_lenth: determine the max lenth of input captions data\n",
        "    self.df = df\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    # We use bert-base-uncased for tokenizing the words\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    # The size of the data\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the images number\n",
        "    row = self.df.iloc[idx]\n",
        "    \n",
        "    # Process image\n",
        "    img_name = os.path.join(self.img_dir, row.ImageID)\n",
        "    # For increasing the running speed and decrease the memory usage, we use only one color channel, not RGB\n",
        "    image = Image.open(img_name).convert(\"RGB\")\n",
        "    # According to the format of 'transform', transform the images data into tensors\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    \n",
        "    # Process text\n",
        "    text = row.Caption\n",
        "    # Tokenize the text\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_length,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True\n",
        "    )\n",
        "    # Get caption number\n",
        "    ids = inputs['input_ids']\n",
        "    # Get the mask\n",
        "    mask = inputs['attention_mask']\n",
        "\n",
        "    # Process labels\n",
        "    # Initialize a list and save the multi-labels\n",
        "    label_indices = list(map(int, row.Labels.strip('[]').split()))\n",
        "    # One-hot the labels. It's very useful when dealing with multi-labels.\n",
        "    # We have labels of 1-19, though we don't have class 0 and 12, we should new a 20-dimentional tensor for processing.\n",
        "    labels = torch.zeros(20)\n",
        "    # Convert labels into binary vectors\n",
        "    labels[label_indices] = 1 \n",
        "\n",
        "    # Processed data\n",
        "    return {\n",
        "      'ids': torch.tensor(ids, dtype=torch.long),\n",
        "      'mask': torch.tensor(mask, dtype=torch.long),\n",
        "      'image': image,\n",
        "      'labels': labels,\n",
        "      'image_names' : img_name\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A. Find the best epochs and optimize the model structure and hyper parameters(pre-trained, learning rate, kernel size, numbers of neurons)."
      ],
      "metadata": {
        "id": "QVcwfXCkL6im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the CNN-Transformer Fusion Model (Resnet and BERT) Class"
      ],
      "metadata": {
        "id": "87xYSQlkSl71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Kz7kRaQi1LLo"
      },
      "outputs": [],
      "source": [
        "class ImageTextModel(nn.Module):\n",
        "  # This class is for combining the image model and the NLP model\n",
        "\n",
        "  def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
        "    # Call initialization\n",
        "    super(ImageTextModel, self).__init__()        \n",
        "    # NLP text model: BERT\n",
        "    self.text_model = BertModel.from_pretrained(bert_model_name)        \n",
        "    # Image model: ResNet (pre-trained)\n",
        "    self.image_model = torchvision.models.resnet50(pretrained=True)\n",
        "    # change the first layer, since we changed the color channel into 1\n",
        "    self.image_model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # Match the fully connected layer with dimension of BERT\n",
        "    self.image_model.fc = nn.Linear(2048, 768)         \n",
        "    # Classification head\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    # Forward propagation, passing data\n",
        "    # Text (vectors)\n",
        "    text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # CLS token\n",
        "    text_features = text_outputs.last_hidden_state[:, 0] \n",
        "\n",
        "    # Images (vectors)\n",
        "    image_features = self.image_model(image)\n",
        "        \n",
        "    # Concatenate features of text and images\n",
        "    combined_features = text_features + image_features\n",
        "    output = self.classifier(combined_features)\n",
        "    # output = self.classifier(image_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define the Training and Testing Function"
      ],
      "metadata": {
        "id": "Tml6nwMcWPt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Training function"
      ],
      "metadata": {
        "id": "AcKJ3gWeWj6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "uza8OwKF1aHP"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  # dataloader: the data in dataloader form\n",
        "  # model: our fusion model\n",
        "  # loss_fn: loss functions for gradient descent\n",
        "  # optimizer: add other optimization parameters, such as learning rate\n",
        "\n",
        "  # Get the size of training set\n",
        "  size = len(dataloader.dataset)\n",
        "  # Start the training\n",
        "  model.train()\n",
        "    \n",
        "  for batch, data in enumerate(dataloader):\n",
        "    # Compute prediction vectors\n",
        "    preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "    # Update the loss\n",
        "    loss = loss_fn(preds, data['labels'].to(device))\n",
        "\n",
        "    # Back propagation, update the weights and bias\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      # Print the loss\n",
        "      loss, current = loss.item(), batch * len(data['image'])\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Testing (predicting) function"
      ],
      "metadata": {
        "id": "MyjltUGsaLlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn, test=True):\n",
        "  # dataloader: test data in dataloader form\n",
        "  # model: our trained fusion model\n",
        "  # loss_fn: loss function\n",
        "  # test: run it on data with or without labels\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  # Initialization\n",
        "  test_loss, correct = 0, 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  all_numbers = []\n",
        "  # Switch to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Traverse process\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      # Make predictions, results are given in vectors form\n",
        "      preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "      all_numbers.append(data['image_names'])\n",
        "      for i in range(len(preds)):\n",
        "        # If we have the labels, calculate the loss and accuracy as reference\n",
        "        if test:\n",
        "          test_loss += loss_fn(preds[i], data['labels'][i].to(device)).item()\n",
        "          # preds[i] > 0 at where the labels are predicted. If all the labels are predicted correctly, it will return the tensor with all True value.\n",
        "          if ((preds[i] > 0) == data['labels'][i].to(device)).all():\n",
        "            correct += 1\n",
        "          all_labels.append(data['labels'][i])\n",
        "        # Transform the predictions result into one-hot form\n",
        "        pred = torch.where(preds[i] < 0, torch.tensor(0), torch.tensor(1))\n",
        "        # Return the data to cpu since they are saved in gpu currently\n",
        "        all_preds.append(pred.cpu())\n",
        "\n",
        "    # Calculate the final results\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # Print the loss and accuracy (0 are shown if we don't have true labels)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    # Return the predictions and true labels for reference\n",
        "    return all_preds, all_labels, all_numbers"
      ],
      "metadata": {
        "id": "xvW8djrJ0QnB"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Main Process: Training"
      ],
      "metadata": {
        "id": "NaX8wA5sdMrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Transfer model training to GPU"
      ],
      "metadata": {
        "id": "ovchxjLngK8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EHOXB6xrBC8g"
      },
      "outputs": [],
      "source": [
        "#The amount of tasks is too large, so we have to try to perform calculations on the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Prepare the data"
      ],
      "metadata": {
        "id": "FgvyBorQgWE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uyHtJaz1hc4",
        "outputId": "081bf978-268a-42ef-95b5-09f399e85fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-125-bf277cce35cd>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
            "Skipping line 9086: expected 3 fields, saw 4\n",
            "Skipping line 9510: expected 3 fields, saw 4\n",
            "Skipping line 18114: expected 3 fields, saw 4\n",
            "Skipping line 27169: expected 3 fields, saw 4\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load the whole training data, drop the bad lines\n",
        "data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
        "\n",
        "# Split the training data for evaluating the model in my environment\n",
        "train_df, test_df = train_test_split(data_given, test_size = 0.2)\n",
        "\n",
        "# Define the transformations, which transform the images data into vectors features\n",
        "transform = transforms.Compose([\n",
        "    # Resize the images\n",
        "    transforms.Resize((256, 256)),\n",
        "    # Randomly apply horizontal flipping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply rotation\n",
        "    transforms.RandomRotation(20),\n",
        "    # Convert PIL image to tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the images (specific values of mean and std are the means and standard deviations of the pytorch.ImageNet dataset)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Train on the whole training set\n",
        "# train_data = ImageTextDataset(data_given, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Train on the 80% of the training set, and test on the other 20% training set\n",
        "train_data = ImageTextDataset(train_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create the model, loss function and optimizer\n",
        "model = ImageTextModel(num_classes=20).to(device)\n",
        "# Use BCEWithLogitsLoss for multi-label classification because it's better for multi-labels task\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# Use the optim.Adam. Learning rate: 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Training"
      ],
      "metadata": {
        "id": "QLE2wrymme12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg8CbxIZmdg8",
        "outputId": "be232051-af98-453b-9d79-0221237a3b81"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.035200  [    0/23996]\n",
            "loss: 0.056670  [ 6400/23996]\n",
            "loss: 0.040780  [12800/23996]\n",
            "loss: 0.048060  [19200/23996]\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation"
      ],
      "metadata": {
        "id": "v5d9hgM1mp0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data as well as create the dataloaders\n",
        "# test_data = ImageTextDataset(test_file, \"/content/5329ASS2/data\", transform=transform)\n",
        "# For validation only\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "qmMfSyfTTsDg"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Make prediction"
      ],
      "metadata": {
        "id": "szEE46yzo7bO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo67XP3h4H2H",
        "outputId": "fa431f33-b557-41d4-d894-b90411d5d074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.079560 \n",
            "\n",
            "6000\n"
          ]
        }
      ],
      "source": [
        "# Acc and loss will be 0 if we don't have true labels of test data\n",
        "pred, rightlabel, all_numbers = test_loop(test_dataloader, model, loss_fn, test=True)\n",
        "print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the predicted labels into numpy form\n",
        "y_pred = [t.numpy() for t in pred]"
      ],
      "metadata": {
        "id": "XLdvUM2ap1Lo"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Calculate the f1 score if we have true labels (validation)"
      ],
      "metadata": {
        "id": "th3cWT2jphyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [t.numpy() for t in rightlabel]\n",
        "f1 = f1_score(y_true, y_pred, average='micro')\n",
        "f1"
      ],
      "metadata": {
        "id": "XK5o7uR22AHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a57dfcd-8273-492d-879f-b66216e51758"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8320864037801654"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B. Use CNN model (ResNet) only"
      ],
      "metadata": {
        "id": "oqT4ZJF8TjDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the CNN Model Resnet only"
      ],
      "metadata": {
        "id": "CCQdXGUnT8z8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NvQ126tcT8z9"
      },
      "outputs": [],
      "source": [
        "class ImageModel(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
        "    # Call initialization\n",
        "    super(ImageModel, self).__init__()               \n",
        "    # Image model: ResNet (pre-trained)\n",
        "    self.image_model = torchvision.models.resnet50(pretrained=True)\n",
        "    # change the first conv1 layer\n",
        "    self.image_model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.image_model.fc = nn.Linear(2048, 768)         \n",
        "    # Classification head\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    # Images (vectors)\n",
        "    image_features = self.image_model(image)\n",
        "    output = self.classifier(image_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Main Process: Training"
      ],
      "metadata": {
        "id": "XKdoNle7T8z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Transfer model training to GPU"
      ],
      "metadata": {
        "id": "zLXFmN9RT8z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Prepare the data"
      ],
      "metadata": {
        "id": "Tz0_7ggIT8z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b923b7f8-bfe8-4469-95be-03c61f5e3bb6",
        "id": "zs1q2i4rT8z-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-7e7ebfdd1c69>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
            "Skipping line 9086: expected 3 fields, saw 4\n",
            "Skipping line 9510: expected 3 fields, saw 4\n",
            "Skipping line 18114: expected 3 fields, saw 4\n",
            "Skipping line 27169: expected 3 fields, saw 4\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 329MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the whole training data, drop the bad lines\n",
        "data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
        "\n",
        "# Split the training data for evaluating the model in my environment\n",
        "train_df, test_df = train_test_split(data_given, test_size = 0.2)\n",
        "\n",
        "# Define the transformations, which transform the images data into vectors features\n",
        "transform = transforms.Compose([\n",
        "    # Resize the images\n",
        "    transforms.Resize((256, 256)),\n",
        "    # Randomly apply horizontal flipping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply rotation\n",
        "    transforms.RandomRotation(20),\n",
        "    # Convert PIL image to tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the images (specific values of mean and std are the means and standard deviations of the pytorch.ImageNet dataset)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Train on the whole training set\n",
        "# train_data = ImageTextDataset(data_given, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Train on the 80% of the training set, and test on the other 20% training set\n",
        "train_data = ImageTextDataset(train_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create the model, loss function and optimizer\n",
        "model = ImageModel(num_classes=20).to(device)\n",
        "# Use BCEWithLogitsLoss for multi-label classification because it's better for multi-labels task\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# Use the optim.Adam. Learning rate: 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Training"
      ],
      "metadata": {
        "id": "YXs_LR_ET8z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b0bc7b-36a3-4e14-ced8-16d91db7a742",
        "id": "tHJXT-5sT8z-"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.028043  [    0/23996]\n",
            "loss: 0.032276  [ 6400/23996]\n",
            "loss: 0.047948  [12800/23996]\n",
            "loss: 0.048229  [19200/23996]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.032780  [    0/23996]\n",
            "loss: 0.019022  [ 6400/23996]\n",
            "loss: 0.036389  [12800/23996]\n",
            "loss: 0.048085  [19200/23996]\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation"
      ],
      "metadata": {
        "id": "X8IJfD8OT8z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data as well as create the dataloaders\n",
        "# test_data = ImageTextDataset(test_file, \"/content/5329ASS2/data\", transform=transform)\n",
        "# For validation only\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "YOJLPVSET8z_"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Make prediction"
      ],
      "metadata": {
        "id": "mDUEQkjzT8z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1495cf17-27f2-4dc3-9433-56930410c7ce",
        "id": "yjmjYt5fT8z_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 57.5%, Avg loss: 0.123323 \n",
            "\n",
            "6000\n"
          ]
        }
      ],
      "source": [
        "# Acc and loss will be 0 if we don't have true labels of test data\n",
        "pred, rightlabel, all_numbers = test_loop(test_dataloader, model, loss_fn, test=True)\n",
        "print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the predicted labels into numpy form\n",
        "y_pred = [t.numpy() for t in pred]"
      ],
      "metadata": {
        "id": "uxxi_WRxT8z_"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Calculate the f1 score if we have true labels (validation)"
      ],
      "metadata": {
        "id": "rEYKMUylT8z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [t.numpy() for t in rightlabel]\n",
        "f1 = f1_score(y_true, y_pred, average='micro')\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd5850a-b7b5-4363-dbfa-978c7e00538f",
        "id": "uwecip15T8z_"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7675891583452211"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compression\n",
        "torch.save(model, 'original_model')\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model, 'quantized_model')"
      ],
      "metadata": {
        "id": "XLeXcMKaicLL"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C. SqueezeNet1.1 with BERT"
      ],
      "metadata": {
        "id": "YiwwjUo0ozoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the CNN-Transformer Fusion Model (SqueezeNet1.1 and BERT) Class"
      ],
      "metadata": {
        "id": "afq6sGBJpIDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "bp47vTTspIDm"
      },
      "outputs": [],
      "source": [
        "class ImageTextModel_new(nn.Module):\n",
        "  # This class is for combining the image model and the NLP model\n",
        "\n",
        "  def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
        "    # Call initialization\n",
        "    super(ImageTextModel_new, self).__init__()        \n",
        "    # NLP text model: BERT\n",
        "    self.text_model = BertModel.from_pretrained(bert_model_name)        \n",
        "    # Image model: SqueezeNet1_1 (pre-trained)\n",
        "    self.image_model = torchvision.models.squeezenet1_1(pretrained=True)\n",
        "    self.image_model.classifier[1] = torch.nn.Conv2d(512, 768, kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "    # Reset final convolution layer's weights\n",
        "    self.image_model.classifier[1].weight.data.normal_(0, 0.01)\n",
        "    self.image_model.classifier[1].bias.data.zero_()\n",
        "    \n",
        "    # Classifier to reduce the combined features to the number of classes\n",
        "    self.classifier = torch.nn.Linear(768*2, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    # Forward propagation, passing data\n",
        "    # Text (vectors)\n",
        "    text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # CLS token\n",
        "    text_features = text_outputs.last_hidden_state[:, 0] \n",
        "\n",
        "    # Images (vectors)\n",
        "    image_features = self.image_model(image)\n",
        "    image_features = image_features.view(image_features.size(0), -1)  # Flattening\n",
        "\n",
        "    # Concatenate features of text and images\n",
        "    combined_features = torch.cat((text_features, image_features), dim=1)\n",
        "    output = self.classifier(combined_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define the Training and Testing Function"
      ],
      "metadata": {
        "id": "LI4AwxeipIDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Training function"
      ],
      "metadata": {
        "id": "WpUUM-1JpIDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "cOv76HITpIDm"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  # dataloader: the data in dataloader form\n",
        "  # model: our fusion model\n",
        "  # loss_fn: loss functions for gradient descent\n",
        "  # optimizer: add other optimization parameters, such as learning rate\n",
        "\n",
        "  # Get the size of training set\n",
        "  size = len(dataloader.dataset)\n",
        "  # Start the training\n",
        "  model.train()\n",
        "    \n",
        "  for batch, data in enumerate(dataloader):\n",
        "    # Compute prediction vectors\n",
        "    preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "    # Update the loss\n",
        "    loss = loss_fn(preds, data['labels'].to(device))\n",
        "\n",
        "    # Back propagation, update the weights and bias\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      # Print the loss\n",
        "      loss, current = loss.item(), batch * len(data['image'])\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Testing (predicting) function"
      ],
      "metadata": {
        "id": "fZTXb-1YpIDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn, test=True):\n",
        "  # dataloader: test data in dataloader form\n",
        "  # model: our trained fusion model\n",
        "  # loss_fn: loss function\n",
        "  # test: run it on data with or without labels\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  # Initialization\n",
        "  test_loss, correct = 0, 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  all_numbers = []\n",
        "  # Switch to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Traverse process\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      # Make predictions, results are given in vectors form\n",
        "      preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "      all_numbers.append(data['image_names'])\n",
        "      for i in range(len(preds)):\n",
        "        # If we have the labels, calculate the loss and accuracy as reference\n",
        "        if test:\n",
        "          test_loss += loss_fn(preds[i], data['labels'][i].to(device)).item()\n",
        "          # preds[i] > 0 at where the labels are predicted. If all the labels are predicted correctly, it will return the tensor with all True value.\n",
        "          if ((preds[i] > 0) == data['labels'][i].to(device)).all():\n",
        "            correct += 1\n",
        "          all_labels.append(data['labels'][i])\n",
        "        # Transform the predictions result into one-hot form\n",
        "        pred = torch.where(preds[i] < 0, torch.tensor(0), torch.tensor(1))\n",
        "        # Return the data to cpu since they are saved in gpu currently\n",
        "        all_preds.append(pred.cpu())\n",
        "\n",
        "    # Calculate the final results\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # Print the loss and accuracy (0 are shown if we don't have true labels)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    # Return the predictions and true labels for reference\n",
        "    return all_preds, all_labels, all_numbers"
      ],
      "metadata": {
        "id": "FxQttsY7pIDn"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Main Process: Training"
      ],
      "metadata": {
        "id": "obLYzpjWpIDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Prepare the data"
      ],
      "metadata": {
        "id": "swIYQSagpIDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448c7701-7807-46c8-8c52-7c24783ab12a",
        "id": "Egpo74AgpIDn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-6f962b6613ae>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
            "Skipping line 9086: expected 3 fields, saw 4\n",
            "Skipping line 9510: expected 3 fields, saw 4\n",
            "Skipping line 18114: expected 3 fields, saw 4\n",
            "Skipping line 27169: expected 3 fields, saw 4\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n",
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 156MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the whole training data, drop the bad lines\n",
        "data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
        "\n",
        "# Split the training data for evaluating the model in my environment\n",
        "train_df, test_df = train_test_split(data_given, test_size = 0.2)\n",
        "\n",
        "# Define the transformations, which transform the images data into vectors features\n",
        "transform = transforms.Compose([\n",
        "    # Resize the images\n",
        "    transforms.Resize((256, 256)),\n",
        "    # Randomly apply horizontal flipping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply rotation\n",
        "    transforms.RandomRotation(20),\n",
        "    # Convert PIL image to tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the images (specific values of mean and std are the means and standard deviations of the pytorch.ImageNet dataset)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Train on the whole training set\n",
        "# train_data = ImageTextDataset(data_given, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Train on the 80% of the training set, and test on the other 20% training set\n",
        "train_data = ImageTextDataset(train_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create the model, loss function and optimizer\n",
        "model = ImageTextModel_new(num_classes=20).to(device)\n",
        "# Use BCEWithLogitsLoss for multi-label classification because it's better for multi-labels task\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# Use the optim.Adam. Learning rate: 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Training"
      ],
      "metadata": {
        "id": "YA0wrhZApIDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6e9a5f-7c01-4fbf-f4fe-f59fb1b067d6",
        "id": "N9YJQEhypIDo"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.056411  [    0/23996]\n",
            "loss: 0.036178  [ 6400/23996]\n",
            "loss: 0.055326  [12800/23996]\n",
            "loss: 0.056350  [19200/23996]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.029216  [    0/23996]\n",
            "loss: 0.021243  [ 6400/23996]\n",
            "loss: 0.041047  [12800/23996]\n",
            "loss: 0.053141  [19200/23996]\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation"
      ],
      "metadata": {
        "id": "M7_siG0ipIDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data as well as create the dataloaders\n",
        "# test_data = ImageTextDataset(test_file, \"/content/5329ASS2/data\", transform=transform)\n",
        "# For validation only\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "XX9q5IDzpIDo"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Make prediction"
      ],
      "metadata": {
        "id": "5oqtJINIpIDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d31171-22a3-4e41-9b73-73231cfe324e",
        "id": "yk-36Mm3pIDo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 65.4%, Avg loss: 0.088095 \n",
            "\n",
            "6000\n"
          ]
        }
      ],
      "source": [
        "# Acc and loss will be 0 if we don't have true labels of test data\n",
        "pred, rightlabel, all_numbers = test_loop(test_dataloader, model, loss_fn, test=True)\n",
        "print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the predicted labels into numpy form\n",
        "y_pred = [t.numpy() for t in pred]"
      ],
      "metadata": {
        "id": "BrySTwwZpIDo"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Calculate the f1 score if we have true labels (validation)"
      ],
      "metadata": {
        "id": "7eKTuQP1pIDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [t.numpy() for t in rightlabel]\n",
        "f1 = f1_score(y_true, y_pred, average='micro')\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9e73ee-1fc8-4a29-9271-4ca2409389d2",
        "id": "vog4HtbMpIDo"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8175322117819164"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compression\n",
        "torch.save(model, 'original_model')\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model, 'quantized_model')"
      ],
      "metadata": {
        "id": "GkFleNp_4adG"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part D. SqueezeNet1.1 only"
      ],
      "metadata": {
        "id": "V5aT2jfgCSpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "cydpCv2CCiTb"
      },
      "outputs": [],
      "source": [
        "class ImageModel_new(nn.Module):\n",
        "  # This class is for combining the image model and the NLP model\n",
        "\n",
        "  def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
        "    # Call initialization\n",
        "    super(ImageModel_new, self).__init__()        \n",
        "       \n",
        "    # Image model: SqueezeNet1_1 (pre-trained)\n",
        "    self.image_model = torchvision.models.squeezenet1_1(pretrained=True)\n",
        "    self.image_model.classifier[1] = torch.nn.Conv2d(512, 768, kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "    # Reset final convolution layer's weights\n",
        "    self.image_model.classifier[1].weight.data.normal_(0, 0.01)\n",
        "    self.image_model.classifier[1].bias.data.zero_()\n",
        "    \n",
        "    # Classifier to reduce the combined features to the number of classes\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    # Forward propagation, passing data\n",
        "\n",
        "\n",
        "    # Images (vectors)\n",
        "    image_features = self.image_model(image)\n",
        "    # image_features = image_features.view(image_features.size(0), -1)  # Flattening\n",
        "\n",
        "\n",
        "    output = self.classifier(image_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a8f94c-ca99-4efd-b17b-f58b703a8f58",
        "id": "zvSP0pkaCiTb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-161-8494e7c8d3c5>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
            "Skipping line 9086: expected 3 fields, saw 4\n",
            "Skipping line 9510: expected 3 fields, saw 4\n",
            "Skipping line 18114: expected 3 fields, saw 4\n",
            "Skipping line 27169: expected 3 fields, saw 4\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load the whole training data, drop the bad lines\n",
        "data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
        "\n",
        "# Split the training data for evaluating the model in my environment\n",
        "train_df, test_df = train_test_split(data_given, test_size = 0.2)\n",
        "\n",
        "# Define the transformations, which transform the images data into vectors features\n",
        "transform = transforms.Compose([\n",
        "    # Resize the images\n",
        "    transforms.Resize((256, 256)),\n",
        "    # Randomly apply horizontal flipping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply rotation\n",
        "    transforms.RandomRotation(20),\n",
        "    # Convert PIL image to tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the images (specific values of mean and std are the means and standard deviations of the pytorch.ImageNet dataset)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Train on the whole training set\n",
        "# train_data = ImageTextDataset(data_given, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Train on the 80% of the training set, and test on the other 20% training set\n",
        "train_data = ImageTextDataset(train_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create the model, loss function and optimizer\n",
        "model = ImageModel_new(num_classes=20).to(device)\n",
        "# Use BCEWithLogitsLoss for multi-label classification because it's better for multi-labels task\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# Use the optim.Adam. Learning rate: 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc37bced-e984-48eb-a74f-cbaf6274da6e",
        "id": "XqQrlMjzCiTb"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.708764  [    0/23996]\n",
            "loss: 0.156727  [ 6400/23996]\n",
            "loss: 0.143899  [12800/23996]\n",
            "loss: 0.128263  [19200/23996]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.092530  [    0/23996]\n",
            "loss: 0.134718  [ 6400/23996]\n",
            "loss: 0.091756  [12800/23996]\n",
            "loss: 0.118144  [19200/23996]\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data as well as create the dataloaders\n",
        "# test_data = ImageTextDataset(test_file, \"/content/5329ASS2/data\", transform=transform)\n",
        "# For validation only\n",
        "test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "nzDscukGCiTb"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27441336-4208-4ff6-a29f-95aedfe36d85",
        "id": "5IiuYTnmCiTc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 52.7%, Avg loss: 0.116757 \n",
            "\n",
            "6000\n"
          ]
        }
      ],
      "source": [
        "# Acc and loss will be 0 if we don't have true labels of test data\n",
        "pred, rightlabel, all_numbers = test_loop(test_dataloader, model, loss_fn, test=True)\n",
        "print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the predicted labels into numpy form\n",
        "y_pred = [t.numpy() for t in pred]"
      ],
      "metadata": {
        "id": "uUa9tAJjCiTc"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [t.numpy() for t in rightlabel]\n",
        "f1 = f1_score(y_true, y_pred, average='micro')\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2bc103-041e-4b02-93d2-fde62f7bad7e",
        "id": "bzjhl9bZCiTc"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7043238270469181"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compression\n",
        "torch.save(model, 'original_model')\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model, 'quantized_model')"
      ],
      "metadata": {
        "id": "gxnvp3BJClZ2"
      },
      "execution_count": 167,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}