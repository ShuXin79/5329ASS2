{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP5329 CNN-Ttransformer Classifier"
      ],
      "metadata": {
        "id": "ZacMxlfJrFpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation and Package Import"
      ],
      "metadata": {
        "id": "wA3IAdLzrWpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Clone data and images from GitHub"
      ],
      "metadata": {
        "id": "TA3iYQB_rkmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFHedtOKKFy",
        "outputId": "47d0d7f7-d049-4f70-a6af-0740453b1c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '5329ASS2'...\n",
            "remote: Enumerating objects: 40017, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 40017 (delta 4), reused 10 (delta 2), pack-reused 40005\u001b[K\n",
            "Receiving objects: 100% (40017/40017), 393.03 MiB | 36.99 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Updating files: 100% (40004/40004), done.\n"
          ]
        }
      ],
      "source": [
        "# All the data were saved in my GitHub, it's easy to get them all by running this code.\n",
        "!git clone https://github.com/ShuXin79/5329ASS2.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Libraries install and import"
      ],
      "metadata": {
        "id": "YvxiD0sdrBZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKFrt_r5LuZY",
        "outputId": "679eef75-4aa5-4c41-f4f8-ab49ffd1c31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "# Make sure your environment contains these libraries.\n",
        "!pip install torch pandas pillow\n",
        "!pip install torchvision\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BxXuABzVi6yf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "# We used the pre-trained model BERT for our NLP task\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "# sklearn is only used for splitting the train-validation sets as well as calculating the f1-score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Reading Class"
      ],
      "metadata": {
        "id": "Vwtf_PzDsAF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vBzfzq5i1Hdi"
      },
      "outputs": [],
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "  # This class is to convert the input data into the format we want to see.\n",
        "  \n",
        "  def __init__(self, df, img_dir, transform=None, max_length=128):\n",
        "    # df: the dataframe of our input data\n",
        "    # img_dir: saved the path to the images\n",
        "    # transform: determine the format of input images data\n",
        "    # max_lenth: determine the max lenth of input captions data\n",
        "    self.df = df\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    # We use bert-base-uncased for tokenizing the words\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    # The size of the data\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get the images number\n",
        "    row = self.df.iloc[idx]\n",
        "    \n",
        "    # Process image\n",
        "    img_name = os.path.join(self.img_dir, row.ImageID)\n",
        "    # For increasing the running speed and decrease the memory usage, we use only one color channel, not RGB\n",
        "    image = Image.open(img_name).convert(\"RGB\")\n",
        "    # According to the format of 'transform', transform the images data into tensors\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    \n",
        "    # Process text\n",
        "    text = row.Caption\n",
        "    # Tokenize the text\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_length,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True\n",
        "    )\n",
        "    # Get caption number\n",
        "    ids = inputs['input_ids']\n",
        "    # Get the mask\n",
        "    mask = inputs['attention_mask']\n",
        "\n",
        "    # Process labels\n",
        "    # Initialize a list and save the multi-labels\n",
        "    label_indices = list(map(int, row.Labels.strip('[]').split()))\n",
        "    # One-hot the labels. It's very useful when dealing with multi-labels.\n",
        "    # We have labels of 1-19, though we don't have class 0 and 12, we should new a 20-dimentional tensor for processing.\n",
        "    labels = torch.zeros(20)\n",
        "    # Convert labels into binary vectors\n",
        "    labels[label_indices] = 1 \n",
        "\n",
        "    # Processed data\n",
        "    return {\n",
        "      'ids': torch.tensor(ids, dtype=torch.long),\n",
        "      'mask': torch.tensor(mask, dtype=torch.long),\n",
        "      'image': image,\n",
        "      'labels': labels,\n",
        "      'image_names' : img_name\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the CNN-Transformer Fusion Model (Resnet and BERT) Class"
      ],
      "metadata": {
        "id": "87xYSQlkSl71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kz7kRaQi1LLo"
      },
      "outputs": [],
      "source": [
        "class ImageTextModel(nn.Module):\n",
        "  # This class is for combining the image model and the NLP model\n",
        "\n",
        "  def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
        "    # Call initialization\n",
        "    super(ImageTextModel, self).__init__()        \n",
        "    # NLP text model: BERT\n",
        "    self.text_model = BertModel.from_pretrained(bert_model_name)        \n",
        "    # Image model: ResNet (pre-trained)\n",
        "    self.image_model = torchvision.models.resnet50(pretrained=True)\n",
        "    # change the first conv1 layer\n",
        "    self.image_model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # Match the fully connected layer with dimension of BERT\n",
        "    self.image_model.fc = nn.Linear(2048, 768)         \n",
        "    # Classification head\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, image):\n",
        "    # Forward propagation, passing data\n",
        "    # Text (vectors)\n",
        "    text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # CLS token\n",
        "    text_features = text_outputs.last_hidden_state[:, 0] \n",
        "\n",
        "    # Images (vectors)\n",
        "    image_features = self.image_model(image)\n",
        "        \n",
        "    # Concatenate features of text and images\n",
        "    combined_features = text_features + image_features\n",
        "    output = self.classifier(combined_features)\n",
        "    # output = self.classifier(image_features)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define the Training and Testing Function"
      ],
      "metadata": {
        "id": "Tml6nwMcWPt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Training function"
      ],
      "metadata": {
        "id": "AcKJ3gWeWj6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uza8OwKF1aHP"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  # dataloader: the data in dataloader form\n",
        "  # model: our fusion model\n",
        "  # loss_fn: loss functions for gradient descent\n",
        "  # optimizer: add other optimization parameters, such as learning rate\n",
        "\n",
        "  # Get the size of training set\n",
        "  size = len(dataloader.dataset)\n",
        "  # Start the training\n",
        "  model.train()\n",
        "    \n",
        "  for batch, data in enumerate(dataloader):\n",
        "    # Compute prediction vectors\n",
        "    preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "    # Update the loss\n",
        "    loss = loss_fn(preds, data['labels'].to(device))\n",
        "\n",
        "    # Back propagation, update the weights and bias\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      # Print the loss\n",
        "      loss, current = loss.item(), batch * len(data['image'])\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Testing (predicting) function"
      ],
      "metadata": {
        "id": "MyjltUGsaLlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn, test=True):\n",
        "  # dataloader: test data in dataloader form\n",
        "  # model: our trained fusion model\n",
        "  # loss_fn: loss function\n",
        "  # test: run it on data with or without labels\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  # Initialization\n",
        "  test_loss, correct = 0, 0\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  all_numbers = []\n",
        "  # Switch to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Traverse process\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      # Make predictions, results are given in vectors form\n",
        "      preds = model(data['ids'].to(device), data['mask'].to(device), data['image'].to(device))\n",
        "      all_numbers.append(data['image_names'])\n",
        "      for i in range(len(preds)):\n",
        "        # If we have the labels, calculate the loss and accuracy as reference\n",
        "        if test:\n",
        "          test_loss += loss_fn(preds[i], data['labels'][i].to(device)).item()\n",
        "          # preds[i] > 0 at where the labels are predicted. If all the labels are predicted correctly, it will return the tensor with all True value.\n",
        "          if ((preds[i] > 0) == data['labels'][i].to(device)).all():\n",
        "            correct += 1\n",
        "          all_labels.append(data['labels'][i])\n",
        "        # Transform the predictions result into one-hot form\n",
        "        pred = torch.where(preds[i] < 0, torch.tensor(0), torch.tensor(1))\n",
        "        # Return the data to cpu since they are saved in gpu currently\n",
        "        all_preds.append(pred.cpu())\n",
        "\n",
        "    # Calculate the final results\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    # Print the loss and accuracy (0 are shown if we don't have true labels)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    # Return the predictions and true labels for reference\n",
        "    return all_preds, all_labels, all_numbers"
      ],
      "metadata": {
        "id": "xvW8djrJ0QnB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Main Process: Training"
      ],
      "metadata": {
        "id": "NaX8wA5sdMrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Transfer model training to GPU"
      ],
      "metadata": {
        "id": "ovchxjLngK8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EHOXB6xrBC8g"
      },
      "outputs": [],
      "source": [
        "#The amount of tasks is too large, so we have to try to perform calculations on the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Prepare the data"
      ],
      "metadata": {
        "id": "FgvyBorQgWE3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uyHtJaz1hc4",
        "outputId": "34d15641-434e-449e-80a7-7fb30315f1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-37a4d0681576>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
            "Skipping line 9086: expected 3 fields, saw 4\n",
            "Skipping line 9510: expected 3 fields, saw 4\n",
            "Skipping line 18114: expected 3 fields, saw 4\n",
            "Skipping line 27169: expected 3 fields, saw 4\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Load the whole training data, drop the bad lines\n",
        "data_given = pd.read_csv(\"/content/5329ASS2/train.csv\", error_bad_lines=False)\n",
        "\n",
        "# Split the training data for evaluating the model in my environment\n",
        "train_df, test_df = train_test_split(data_given, test_size = 0.2)\n",
        "\n",
        "# Define the transformations, which transform the images data into vectors features\n",
        "transform = transforms.Compose([\n",
        "    # Resize the images\n",
        "    transforms.Resize((256, 256)),\n",
        "    # Randomly apply horizontal flipping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply rotation\n",
        "    transforms.RandomRotation(20),\n",
        "    # Convert PIL image to tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the images (specific values of mean and std are the means and standard deviations of the pytorch.ImageNet dataset)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Train on the whole training set\n",
        "train_data = ImageTextDataset(data_given, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Train on the 80% of the training set, and test on the other 20% training set\n",
        "# train_data = ImageTextDataset(train_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "# test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create the model, loss function and optimizer\n",
        "model = ImageTextModel(num_classes=20).to(device)\n",
        "# Use BCEWithLogitsLoss for multi-label classification because it's better for multi-labels task\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# Use the optim.Adam. Learning rate: 0.0001 Weight decay: 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Training"
      ],
      "metadata": {
        "id": "QLE2wrymme12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "epochs = 4\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Model training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg8CbxIZmdg8",
        "outputId": "05f420cd-accb-4113-8bb2-0d94fa845fd1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.659498  [    0/29996]\n",
            "loss: 0.091571  [ 6400/29996]\n",
            "loss: 0.074293  [12800/29996]\n",
            "loss: 0.087336  [19200/29996]\n",
            "loss: 0.053123  [25600/29996]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.061503  [    0/29996]\n",
            "loss: 0.091093  [ 6400/29996]\n",
            "loss: 0.055099  [12800/29996]\n",
            "loss: 0.066847  [19200/29996]\n",
            "loss: 0.059706  [25600/29996]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.071786  [    0/29996]\n",
            "loss: 0.081639  [ 6400/29996]\n",
            "loss: 0.042450  [12800/29996]\n",
            "loss: 0.041498  [19200/29996]\n",
            "loss: 0.091622  [25600/29996]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.058976  [    0/29996]\n",
            "loss: 0.078112  [ 6400/29996]\n",
            "loss: 0.058881  [12800/29996]\n",
            "loss: 0.035179  [19200/29996]\n",
            "loss: 0.067312  [25600/29996]\n",
            "Model training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation"
      ],
      "metadata": {
        "id": "v5d9hgM1mp0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Prepare the test data"
      ],
      "metadata": {
        "id": "Yqf1UV3fn_Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data, drop the bad lines temporarily\n",
        "\n",
        "test_file = pd.read_csv('/content/5329ASS2/test.csv', error_bad_lines = False)\n",
        "\n",
        "# New a column 'Labels' for avoiding bug\n",
        "test_file['Labels'] = '0'\n",
        "test_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "IvQfrI-_LiGq",
        "outputId": "1c3b652d-415b-4242-8629-aa75eefe9866"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-6e7fcfd880de>:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  test_file = pd.read_csv('/content/5329ASS2/test.csv', error_bad_lines = False)\n",
            "Skipping line 6891: expected 2 fields, saw 3\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ImageID                                            Caption Labels\n",
              "0     30000.jpg  A little girl waring a krispy kreme hat holdin...      0\n",
              "1     30001.jpg  A beautiful young woman holding an orange fris...      0\n",
              "2     30002.jpg  A group of people sitting on couch next to a c...      0\n",
              "3     30003.jpg         A person on a snowboard rides on the hill.      0\n",
              "4     30004.jpg  A man riding a skateboard with a helmet on in ...      0\n",
              "...         ...                                                ...    ...\n",
              "9994  39995.jpg  A group of men riding surfboards riding a mass...      0\n",
              "9995  39996.jpg  A motorcycle parked next to a car in a parking...      0\n",
              "9996  39997.jpg            a little boy that is playing with a wii      0\n",
              "9997  39998.jpg  group of kids play Frisbee golf in the middle ...      0\n",
              "9998  39999.jpg   A man in a gray jacket standing next to a woman.      0\n",
              "\n",
              "[9999 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b48dd19b-1a9c-4d39-af94-cd294ad55997\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageID</th>\n",
              "      <th>Caption</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30000.jpg</td>\n",
              "      <td>A little girl waring a krispy kreme hat holdin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30001.jpg</td>\n",
              "      <td>A beautiful young woman holding an orange fris...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30002.jpg</td>\n",
              "      <td>A group of people sitting on couch next to a c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30003.jpg</td>\n",
              "      <td>A person on a snowboard rides on the hill.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30004.jpg</td>\n",
              "      <td>A man riding a skateboard with a helmet on in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>39995.jpg</td>\n",
              "      <td>A group of men riding surfboards riding a mass...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>39996.jpg</td>\n",
              "      <td>A motorcycle parked next to a car in a parking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>39997.jpg</td>\n",
              "      <td>a little boy that is playing with a wii</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>39998.jpg</td>\n",
              "      <td>group of kids play Frisbee golf in the middle ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>39999.jpg</td>\n",
              "      <td>A man in a gray jacket standing next to a woman.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9999 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b48dd19b-1a9c-4d39-af94-cd294ad55997')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b48dd19b-1a9c-4d39-af94-cd294ad55997 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b48dd19b-1a9c-4d39-af94-cd294ad55997');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill bad lines manually ---- this is the easiest way\n",
        "pd_arr1 = test_file[:6889]\n",
        "pd_arr2 = test_file[6889:]\n",
        "pd_arr1.loc[6889] = ['36889.jpg', 'Stop sign with added war\" annotation at an intersection.\"', '0']\n",
        "test_file = pd_arr1.append(pd_arr2, ignore_index=True)\n",
        "test_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "dZMHpvqDTlq7",
        "outputId": "bd8af635-12cf-47a2-df3f-45faf0f4021a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-a9f3834ca561>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pd_arr1.loc[6889] = ['36889.jpg', 'Stop sign with added war\" annotation at an intersection.\"', '0']\n",
            "<ipython-input-58-a9f3834ca561>:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_file = pd_arr1.append(pd_arr2, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ImageID                                            Caption Labels\n",
              "0     30000.jpg  A little girl waring a krispy kreme hat holdin...      0\n",
              "1     30001.jpg  A beautiful young woman holding an orange fris...      0\n",
              "2     30002.jpg  A group of people sitting on couch next to a c...      0\n",
              "3     30003.jpg         A person on a snowboard rides on the hill.      0\n",
              "4     30004.jpg  A man riding a skateboard with a helmet on in ...      0\n",
              "...         ...                                                ...    ...\n",
              "9995  39995.jpg  A group of men riding surfboards riding a mass...      0\n",
              "9996  39996.jpg  A motorcycle parked next to a car in a parking...      0\n",
              "9997  39997.jpg            a little boy that is playing with a wii      0\n",
              "9998  39998.jpg  group of kids play Frisbee golf in the middle ...      0\n",
              "9999  39999.jpg   A man in a gray jacket standing next to a woman.      0\n",
              "\n",
              "[10000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4447993-ef3d-4b1a-a668-ca6e856dd954\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageID</th>\n",
              "      <th>Caption</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30000.jpg</td>\n",
              "      <td>A little girl waring a krispy kreme hat holdin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30001.jpg</td>\n",
              "      <td>A beautiful young woman holding an orange fris...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30002.jpg</td>\n",
              "      <td>A group of people sitting on couch next to a c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30003.jpg</td>\n",
              "      <td>A person on a snowboard rides on the hill.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30004.jpg</td>\n",
              "      <td>A man riding a skateboard with a helmet on in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>39995.jpg</td>\n",
              "      <td>A group of men riding surfboards riding a mass...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>39996.jpg</td>\n",
              "      <td>A motorcycle parked next to a car in a parking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>39997.jpg</td>\n",
              "      <td>a little boy that is playing with a wii</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>39998.jpg</td>\n",
              "      <td>group of kids play Frisbee golf in the middle ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>39999.jpg</td>\n",
              "      <td>A man in a gray jacket standing next to a woman.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4447993-ef3d-4b1a-a668-ca6e856dd954')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4447993-ef3d-4b1a-a668-ca6e856dd954 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4447993-ef3d-4b1a-a668-ca6e856dd954');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data as well as create the dataloaders\n",
        "test_data = ImageTextDataset(test_file, \"/content/5329ASS2/data\", transform=transform)\n",
        "# For validation only\n",
        "# test_data = ImageTextDataset(test_df, \"/content/5329ASS2/data\", transform=transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "qmMfSyfTTsDg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Make prediction"
      ],
      "metadata": {
        "id": "szEE46yzo7bO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo67XP3h4H2H",
        "outputId": "521c856c-a9d0-419d-d87b-ac460192dfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 0.0%, Avg loss: 0.000000 \n",
            "\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "# Acc and loss will be 0 if we don't have true labels of test data\n",
        "pred, rightlabel, all_numbers = test_loop(test_dataloader, model, loss_fn, test=False)\n",
        "print(len(pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the predicted labels into numpy form\n",
        "y_pred = [t.numpy() for t in pred]"
      ],
      "metadata": {
        "id": "XLdvUM2ap1Lo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Calculate the f1 score if we have true labels (validation)"
      ],
      "metadata": {
        "id": "th3cWT2jphyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = [t.numpy() for t in rightlabel]\n",
        "f1 = f1_score(y_true, y_pred, average='micro')\n",
        "f1"
      ],
      "metadata": {
        "id": "XK5o7uR22AHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7d28c7-0848-4c6d-f6d5-b65a07da9bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411636522118109"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Export output"
      ],
      "metadata": {
        "id": "jwxxQyGWqKXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Get ImageID"
      ],
      "metadata": {
        "id": "kKKWUp1dUzUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path of the picture is stored in class_numbers, we need to remove the path and only keep the ImageID\n",
        "image_path = []\n",
        "for batch in all_numbers:\n",
        "  image_path.append([os.path.basename(path) for path in batch])\n",
        "image_names = [item for sublist in image_path for item in sublist]"
      ],
      "metadata": {
        "id": "PdKYAcyhnSX1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Restore labels format"
      ],
      "metadata": {
        "id": "bArOmQNNqXJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we save the predict labels as one-hot format, we should transform them to original format\n",
        "y_pred_indices = [np.where(arr==1)[0] for arr in y_pred]\n",
        "y_pred_strings = [' '.join(map(str, arr)) for arr in y_pred_indices]\n",
        "y_pred_strings[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQeuCDrrIxk3",
        "outputId": "1111d462-3278-440b-d074-a673a0ddc0c2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7', '1', '1', '1 3 8', '4', '1', '1 7', '1', '1 15', '1']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Export .csv file that saves the labels of the test set"
      ],
      "metadata": {
        "id": "DHdnKplgqxOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Cr09L0OBHblT",
        "outputId": "0b7c9df9-4756-4c9b-8421-92942f7186a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ImageID   Labels\n",
              "892   30000.jpg        1\n",
              "4238  30001.jpg      1 3\n",
              "76    30002.jpg        1\n",
              "9177  30003.jpg        1\n",
              "4211  30004.jpg      1 3\n",
              "...         ...      ...\n",
              "5519  39995.jpg        1\n",
              "2084  39996.jpg  1 3 4 8\n",
              "6128  39997.jpg        1\n",
              "4871  39998.jpg        1\n",
              "8902  39999.jpg        1\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97d25fcc-cad9-4908-b574-1e9975398332\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageID</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>30000.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4238</th>\n",
              "      <td>30001.jpg</td>\n",
              "      <td>1 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>30002.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9177</th>\n",
              "      <td>30003.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4211</th>\n",
              "      <td>30004.jpg</td>\n",
              "      <td>1 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5519</th>\n",
              "      <td>39995.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2084</th>\n",
              "      <td>39996.jpg</td>\n",
              "      <td>1 3 4 8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6128</th>\n",
              "      <td>39997.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4871</th>\n",
              "      <td>39998.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8902</th>\n",
              "      <td>39999.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97d25fcc-cad9-4908-b574-1e9975398332')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97d25fcc-cad9-4908-b574-1e9975398332 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97d25fcc-cad9-4908-b574-1e9975398332');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# Write the ImageID and Labels into one dataframe\n",
        "predict_output = pd.DataFrame({'ImageID': image_names, 'Labels': y_pred_strings})\n",
        "\n",
        "# Arrange items correctly according to ImageID (remove '.jpg', then sort, then add '.jpg')\n",
        "predict_output['ImageID'] = predict_output['ImageID'].str.extract('(\\d+)').astype(int)\n",
        "predict_output = predict_output.sort_values('ImageID')\n",
        "predict_output['ImageID'] = predict_output['ImageID'].astype(str) + '.jpg'\n",
        "\n",
        "# Export .csv file\n",
        "predict_output.to_csv('predict_output.csv', index=False)\n",
        "predict_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Export the model"
      ],
      "metadata": {
        "id": "Sq9XSQ29rD7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MIsJVtoqWs1U"
      },
      "outputs": [],
      "source": [
        "# Model compression\n",
        "torch.save(model, 'original_model')\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.save(quantized_model, 'mymodel')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}